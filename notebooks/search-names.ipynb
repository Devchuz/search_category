{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importacion de liberias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.retrievers import BM25Retriever, QdrantSparseVectorRetriever\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from langchain.retrievers import EnsembleRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Tuple\n",
    "\n",
    "from qdrant_client.models import (\n",
    "    Distance,\n",
    "    NamedSparseVector,\n",
    "    NamedVector,\n",
    "    SparseVector,\n",
    "    PointStruct,\n",
    "    SearchRequest,\n",
    "    SparseIndexParams,\n",
    "    SparseVectorParams,\n",
    "    VectorParams,\n",
    "    ScoredPoint,\n",
    ")\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import fastembed\n",
    "from fastembed import SparseEmbedding, SparseTextEmbedding, TextEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_21824\\354263751.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_category = grouped.apply(lambda x: x.sample(min(len(x), 20))).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Definir la ruta relativa desde la ubicación de tu notebook\n",
    "path = os.path.join('..', 'data')\n",
    "\n",
    "# Verificar si la ruta existe y es una carpeta\n",
    "if os.path.exists(path) and os.path.isdir(path):\n",
    "    # Lista para almacenar los DataFrames\n",
    "    dataframes = []\n",
    "    \n",
    "    # Recorrer la carpeta y leer archivos CSV\n",
    "    for file in os.listdir(path):\n",
    "        file_path = os.path.join(path, file)\n",
    "        if file.endswith('.csv') and os.path.isfile(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            dataframes.append(df)\n",
    "    \n",
    "    # Concatenar todos los DataFrames en uno solo\n",
    "    concatenated_df = pd.concat(dataframes, ignore_index=True)\n",
    "    # Asegurarse de que el DataFrame tiene una columna de categoría\n",
    "    if 'main_category' in concatenated_df.columns:\n",
    "        # Filtrar 1000 filas con 10 categorías distintas\n",
    "        grouped = concatenated_df.groupby('main_category')\n",
    "        df_category = grouped.apply(lambda x: x.sample(min(len(x), 20))).reset_index(drop=True)\n",
    "        \n",
    "        # Si hay más de 1000 filas, tomar una muestra de 200\n",
    "        if len(df_category) > 1000:\n",
    "            df_category = df_category.sample(200).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = pd.DataFrame()\n",
    "df_category['combined_text'] = (df_category['main_category'] + \"\\n\"+ df_category['sub_category'])\n",
    "sampled_df['combined_text'] = df_category['combined_text'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df['id'] = sampled_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>combined_text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accessories\\nGold &amp; Diamond Jewellery</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accessories\\nBags &amp; Luggage</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accessories\\nWatches</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>accessories\\nFashion &amp; Silver Jewellery</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accessories\\nJewellery</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>women's clothing\\nEthnic Wear</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>women's clothing\\nLingerie &amp; Nightwear</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>women's shoes\\nShoes</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>women's shoes\\nFashion Sandals</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>women's shoes\\nBallerinas</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              combined_text  id\n",
       "0     accessories\\nGold & Diamond Jewellery   0\n",
       "1               accessories\\nBags & Luggage   1\n",
       "2                      accessories\\nWatches   2\n",
       "3   accessories\\nFashion & Silver Jewellery   3\n",
       "4                    accessories\\nJewellery   4\n",
       "..                                      ...  ..\n",
       "93            women's clothing\\nEthnic Wear  93\n",
       "94   women's clothing\\nLingerie & Nightwear  94\n",
       "95                     women's shoes\\nShoes  95\n",
       "96           women's shoes\\nFashion Sandals  96\n",
       "97                women's shoes\\nBallerinas  97\n",
       "\n",
       "[98 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_21824\\1769252562.py:4: DeprecationWarning: The right spelling is prithivida/Splade_PP_en_v1. Support of this name will be removed soon, please fix the model_name\n",
      "  sparse_model = SparseTextEmbedding(model_name=sparse_model_name, batch_size=32)\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<?, ?it/s]\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "sparse_model_name = \"prithvida/Splade_PP_en_v1\"\n",
    "dense_model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "# This triggers the model download\n",
    "sparse_model = SparseTextEmbedding(model_name=sparse_model_name, batch_size=32)\n",
    "dense_model = TextEmbedding(model_name=dense_model_name, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sparse_embedding(texts: List[str]):\n",
    "    return list(sparse_model.embed(texts, batch_size=32))\n",
    "\n",
    "\n",
    "def get_tokens_and_weights(sparse_embedding, model_name):\n",
    "    # Find the tokenizer for the model\n",
    "    tokenizer_source = None\n",
    "    for model_info in SparseTextEmbedding.list_supported_models():\n",
    "        if model_info[\"model\"].lower() == model_name.lower():\n",
    "            tokenizer_source = model_info[\"sources\"][\"hf\"]\n",
    "            break\n",
    "        else:\n",
    "            raise ValueError(f\"Model {model_name} not found in the supported models.\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_source)\n",
    "    token_weight_dict = {}\n",
    "    for i in range(len(sparse_embedding.indices)):\n",
    "        token = tokenizer.decode([sparse_embedding.indices[i]])\n",
    "        weight = sparse_embedding.values[i]\n",
    "        token_weight_dict[token] = weight\n",
    "\n",
    "    # Sort the dictionary by weights\n",
    "    token_weight_dict = dict(\n",
    "        sorted(token_weight_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "    )\n",
    "    return token_weight_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dense_embedding(texts: List[str]):\n",
    "    return list(dense_model.embed(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtencion de Sparce Vector and Embbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_texts = sampled_df[\"combined_text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df[\"sparse_embedding\"] = make_sparse_embedding(product_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df[\"dense_embedding\"] = make_dense_embedding(product_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conect to vectordatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conectar a Qdrant\n",
    "# Obtener las variables de entorno\n",
    "QDRANT_URL = os.getenv('QDRANT_URL')\n",
    "QDRANT_API_KEY = os.getenv('QDRANT_API_KEY')\n",
    "# Inicializar el cliente de Qdrant\n",
    "client = QdrantClient(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "# Crear una colección en Qdrant\n",
    "collection_name = \"hybrid_search\"\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name,\n",
    "    vectors_config={\n",
    "        \"text-dense\": VectorParams(\n",
    "            size=1024,  # OpenAI Embeddings\n",
    "            distance=Distance.COSINE,\n",
    "        )\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"text-sparse\": SparseVectorParams(\n",
    "            index=SparseIndexParams(\n",
    "                on_disk=False,\n",
    "            )\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generacion de puntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_points(df: pd.DataFrame) -> List[PointStruct]:\n",
    "    sparse_vectors = df[\"sparse_embedding\"].tolist()\n",
    "    product_texts = df[\"combined_text\"].tolist()\n",
    "    dense_vectors = df[\"dense_embedding\"].tolist()\n",
    "    rows = df.to_dict(orient=\"records\")\n",
    "    points = []\n",
    "    for idx, (text, sparse_vector, dense_vector) in enumerate(\n",
    "        zip(product_texts, sparse_vectors, dense_vectors)\n",
    "    ):\n",
    "        sparse_vector = SparseVector(\n",
    "            indices=sparse_vector.indices.tolist(), values=sparse_vector.values.tolist()\n",
    "        )\n",
    "        point = PointStruct(\n",
    "            id=idx,\n",
    "            payload={\n",
    "                \"text\": text,\n",
    "                \"id\": rows[idx][\"id\"],\n",
    "            },  # Add any additional payload if necessary\n",
    "            vector={\n",
    "                \"text-sparse\": sparse_vector,\n",
    "                \"text-dense\": dense_vector.tolist(),\n",
    "            },\n",
    "        )\n",
    "        points.append(point)\n",
    "    return points\n",
    "\n",
    "\n",
    "points: List[PointStruct] = make_points(sampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.upsert(collection_name, points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "def search(query_text: str):\n",
    "    # Generar vectores esparcidos (sparse) y densos (dense)\n",
    "    query_sparse_vectors: List[SparseEmbedding] = make_sparse_embedding([query_text])\n",
    "    query_dense_vector: List[np.ndarray] = make_dense_embedding([query_text])\n",
    "\n",
    "    # Verificar que los vectores no estén vacíos\n",
    "    if not query_sparse_vectors or not query_dense_vector:\n",
    "        raise ValueError(\"Los vectores generados están vacíos.\")\n",
    "\n",
    "    # Ejecutar la búsqueda con los vectores generados\n",
    "    search_results = client.search_batch(\n",
    "        collection_name=collection_name,\n",
    "        requests=[\n",
    "            SearchRequest(\n",
    "                vector=NamedVector(\n",
    "                    name=\"text-dense\",\n",
    "                    vector=query_dense_vector[0].tolist(),  # Convertir el vector en una lista\n",
    "                ),\n",
    "                limit=10,\n",
    "                with_payload=True,\n",
    "            ),\n",
    "            SearchRequest(\n",
    "                vector=NamedSparseVector(\n",
    "                    name=\"text-sparse\",\n",
    "                    vector=SparseVector(\n",
    "                        indices=query_sparse_vectors[0].indices.tolist(),  # Convertir índices en una lista\n",
    "                        values=query_sparse_vectors[0].values.tolist(),    # Convertir valores en una lista\n",
    "                    ),\n",
    "                ),\n",
    "                limit=10,\n",
    "                with_payload=True,\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return search_results\n",
    "\n",
    "# Ejemplo de uso\n",
    "query_text = \"Naturelix Detox Bath Natural Dog\"\n",
    "search_results = search(query_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ScoredPoint(id=22, version=0, score=0.5735021308119368, payload={'text': 'beauty & health\\nPersonal Care Appliances', 'id': 22}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=20, version=0, score=0.5615359937387494, payload={'text': 'beauty & health\\nBeauty & Grooming', 'id': 20}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=19, version=0, score=0.5604631010204252, payload={'text': 'beauty & health\\nHousehold Supplies', 'id': 19}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=21, version=0, score=0.5583277133834655, payload={'text': 'beauty & health\\nLuxury Beauty', 'id': 21}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=42, version=0, score=0.5547079140584126, payload={'text': 'home, kitchen, pets\\nRefurbished & Open Box', 'id': 42}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=79, version=0, score=0.5539104130258146, payload={'text': 'toys & baby products\\nBaby Bath, Skin & Grooming', 'id': 79}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=18, version=0, score=0.5538778324409626, payload={'text': 'beauty & health\\nHealth & Personal Care', 'id': 18}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=62, version=0, score=0.5504990002579202, payload={'text': 'pet supplies\\nDog supplies', 'id': 62}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=61, version=0, score=0.5245793332026688, payload={'text': 'pet supplies\\nAll Pet Supplies', 'id': 61}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=17, version=0, score=0.49276930207745034, payload={'text': 'beauty & health\\nMake-up', 'id': 17}, vector=None, shard_key=None, order_value=None)],\n",
       " [ScoredPoint(id=62, version=0, score=6.9046454429626465, payload={'text': 'pet supplies\\nDog supplies', 'id': 62}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=61, version=0, score=5.078445911407471, payload={'text': 'pet supplies\\nAll Pet Supplies', 'id': 61}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=79, version=0, score=3.569088935852051, payload={'text': 'toys & baby products\\nBaby Bath, Skin & Grooming', 'id': 79}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=42, version=0, score=3.2715227603912354, payload={'text': 'home, kitchen, pets\\nRefurbished & Open Box', 'id': 42}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=7, version=0, score=1.940529704093933, payload={'text': 'appliances\\nWashing Machines', 'id': 7}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=63, version=0, score=0.907814621925354, payload={'text': 'sports & fitness\\nAll Sports, Fitness & Outdoors', 'id': 63}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=88, version=0, score=0.31412020325660706, payload={'text': 'tv, audio & cameras\\nAll Electronics', 'id': 88}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=38, version=0, score=0.27478480339050293, payload={'text': 'home & kitchen\\nBedroom Linen', 'id': 38}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=5, version=0, score=0.14231164753437042, payload={'text': 'accessories\\nHandbags & Clutches', 'id': 5}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=15, version=0, score=0.11199413985013962, payload={'text': 'bags & luggage\\nTravel Accessories', 'id': 15}, vector=None, shard_key=None, order_value=None)]]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf(rank_lists, alpha=60, default_rank=1000):\n",
    "    \"\"\"\n",
    "    Optimized Reciprocal Rank Fusion (RRF) using NumPy for large rank lists.\n",
    "\n",
    "    :param rank_lists: A list of rank lists. Each rank list should be a list of (item, rank) tuples.\n",
    "    :param alpha: The parameter alpha used in the RRF formula. Default is 60.\n",
    "    :param default_rank: The default rank assigned to items not present in a rank list. Default is 1000.\n",
    "    :return: Sorted list of items based on their RRF scores.\n",
    "    \"\"\"\n",
    "    # Consolidate all unique items from all rank lists\n",
    "    all_items = set(item for rank_list in rank_lists for item, _ in rank_list)\n",
    "\n",
    "    # Create a mapping of items to indices\n",
    "    item_to_index = {item: idx for idx, item in enumerate(all_items)}\n",
    "\n",
    "    # Initialize a matrix to hold the ranks, filled with the default rank\n",
    "    rank_matrix = np.full((len(all_items), len(rank_lists)), default_rank)\n",
    "\n",
    "    # Fill in the actual ranks from the rank lists\n",
    "    for list_idx, rank_list in enumerate(rank_lists):\n",
    "        for item, rank in rank_list:\n",
    "            rank_matrix[item_to_index[item], list_idx] = rank\n",
    "\n",
    "    # Calculate RRF scores using NumPy operations\n",
    "    rrf_scores = np.sum(1.0 / (alpha + rank_matrix), axis=1)\n",
    "\n",
    "    # Sort items based on RRF scores\n",
    "    sorted_indices = np.argsort(-rrf_scores)  # Negative for descending order\n",
    "\n",
    "    # Retrieve sorted items\n",
    "    sorted_items = [(list(item_to_index.keys())[idx], rrf_scores[idx]) for idx in sorted_indices]\n",
    "\n",
    "    return sorted_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(62, 0.031099324975891997),\n",
       " (79, 0.031024531024531024),\n",
       " (42, 0.031009615384615385),\n",
       " (61, 0.030621785881252923),\n",
       " (22, 0.017336838849365915),\n",
       " (20, 0.01707242848447961),\n",
       " (19, 0.016816412099430966),\n",
       " (21, 0.016568396226415094),\n",
       " (7, 0.01632801161103048),\n",
       " (63, 0.016094911377930246),\n",
       " (18, 0.015868769360743454),\n",
       " (88, 0.015868769360743454),\n",
       " (38, 0.01564927857935627),\n",
       " (5, 0.0154361498496035),\n",
       " (15, 0.01522911051212938),\n",
       " (17, 0.01522911051212938)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rank_list(search_result: List[ScoredPoint]):\n",
    "    return [(point.id, rank + 1) for rank, point in enumerate(search_result)]\n",
    "\n",
    "\n",
    "dense_rank_list, sparse_rank_list = rank_list(search_results[0]), rank_list(search_results[1])\n",
    "rrf_rank_list = rrf([dense_rank_list, sparse_rank_list])\n",
    "rrf_rank_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Record(id=62, payload={'text': 'pet supplies\\nDog supplies', 'id': 62}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id=79, payload={'text': 'toys & baby products\\nBaby Bath, Skin & Grooming', 'id': 79}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id=42, payload={'text': 'home, kitchen, pets\\nRefurbished & Open Box', 'id': 42}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id=61, payload={'text': 'pet supplies\\nAll Pet Supplies', 'id': 61}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id=22, payload={'text': 'beauty & health\\nPersonal Care Appliances', 'id': 22}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id=20, payload={'text': 'beauty & health\\nBeauty & Grooming', 'id': 20}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id=19, payload={'text': 'beauty & health\\nHousehold Supplies', 'id': 19}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id=21, payload={'text': 'beauty & health\\nLuxury Beauty', 'id': 21}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id=7, payload={'text': 'appliances\\nWashing Machines', 'id': 7}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id=63, payload={'text': 'sports & fitness\\nAll Sports, Fitness & Outdoors', 'id': 63}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id=18, payload={'text': 'beauty & health\\nHealth & Personal Care', 'id': 18}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id=88, payload={'text': 'tv, audio & cameras\\nAll Electronics', 'id': 88}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id=38, payload={'text': 'home & kitchen\\nBedroom Linen', 'id': 38}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id=5, payload={'text': 'accessories\\nHandbags & Clutches', 'id': 5}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id=15, payload={'text': 'bags & luggage\\nTravel Accessories', 'id': 15}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id=17, payload={'text': 'beauty & health\\nMake-up', 'id': 17}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_point_by_id(\n",
    "    client: QdrantClient, collection_name: str, rrf_rank_list: List[Tuple[int, float]]\n",
    "):\n",
    "    return client.retrieve(\n",
    "        collection_name=collection_name, ids=[item[0] for item in rrf_rank_list]\n",
    "    )\n",
    "\n",
    "\n",
    "find_point_by_id(client, collection_name, rrf_rank_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
